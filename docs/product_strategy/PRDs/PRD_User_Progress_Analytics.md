### **PRD: User Progress Analytics**

### **1. Overview**

This document outlines the product requirements for the User Progress Analytics feature. This feature is pivotal in empowering learners by providing transparent, actionable insights into their comprehension, skill acquisition, and overall learning journey. By giving learners a clear, data-backed understanding of their progress, particularly their ability to practically apply knowledge, we aim to build profound confidence and foster a more effective, motivating learning environment.

### **2. The Problem**

Learners frequently struggle with ambiguity regarding their true mastery of a subject. Traditional metrics like chapter completion or simple test scores fail to pinpoint specific weaknesses, show trends over time, or build confidence in real-world capabilities. This leaves learners feeling uncertain about their competence, unmotivated by unseen progress, and unsure of the most effective next step to take. This feature directly attacks that uncertainty.

### **3. Key Terms & Definitions**

To ensure clarity across all teams, the following terms are defined:

- **Learning Tiers:** A classification of learning activities based on their cognitive purpose, moving a learner from foundational knowledge to real-world application.
    
    - **Tier 1 (Cued Recall):** Demonstrating foundational knowledge. This tier answers the question, "Do I know the basic facts?"
        
        - Examples: Foundational knowledge checks, flashcards.
            
    - **Tier 2 (Conceptual Understanding):** Explaining a concept in simple terms to prove comprehension. This tier answers the question, "Can I explain this to a five-year-old?"
        
        - Examples: Short-answer questions requiring interpretation, labeling interactive diagrams, concept mapping.
            
    - **Tier 3 (Context & Experimentation):** Understanding the "why" behind a concept and observing its effects. This tier answers the question, "Why does this matter, and how does it work in a controlled setting?"
        
        - Examples: Interactive exercises where users manipulate variables; explaining the practical importance of a concept.
            
    - **Tier 4 (Practical Application):** Applying knowledge to solve a realistic problem. This tier answers the question, "Can I actually use this?"
        
        - Examples: Completing a micro-scenario that simulates a real-world challenge, requiring the use of one or more concepts to achieve a goal.
        
- **Gist Rating:** A 1-10 score generated by our Performance Assessment Engine at the conclusion of a Tier 4 scenario. It holistically measures how effectively the user accomplished the scenario's primary goal, regardless of the exact path taken.
    
- **Concept:** The most granular unit of learning in our system (e.g., "Newton's Third Law," "CSS Flexbox," "Cross-Site Scripting"). All learning activities are tagged with one or more concepts.
    
- **Mastery Score:** A calculated, time-weighted score from 0-100 that represents our current best estimate of a learner's proficiency in a single Concept. **This is a V1 hypothesis and will be subject to continuous validation and tuning.**
    

### **4. Goals & Business Impact**

#### **4.1. Primary Goals**

- **User Insight:** Provide learners with transparent, actionable insights into their progress and mastery, fostering confidence and guiding efficient learning.
    
- **User Motivation:** Enhance user engagement and motivation by clearly visualizing their learning journey, making progress tangible.
    
- **Platform Intelligence:** Fuel our adaptive learning engine with granular, real-time performance data to improve content recommendations.
    

#### **4.2. Business Impact**

- **Increase User Retention:** By clarifying the value of continued practice, we aim to increase 30-day active user retention.
    
- **Strengthen Market Position:** Serve as a key differentiator, positioning our platform as a tool for achieving demonstrable competence.
    
- **Enable Premium Tiers:** The insights generated will be a cornerstone for future premium and enterprise-level offerings.
    

### **5. User Stories & User Flow**

#### **5.1. Key User Stories**

- As a learner, I want to see my overall progress at a glance so that I can feel a sense of momentum.
    
- As a learner, I want to instantly identify my specific strengths and knowledge gaps so that I know what to focus on.
    
- As a learner, I want to see the evidence behind the system's assessment of my skills so that I can trust its analysis.
    
- As a learner, I want to get clear, actionable recommendations to improve efficiently.
    
- As a learner, I want to see my practical application ability (Gist Rating) prominently displayed to confirm my real-world competence.
    

#### **5.2. High-Level User Flow**

1. **Access Dashboard:** User clicks a "My Progress" link in the main navigation.
    
2. **View Overview:** The dashboard loads, displaying:
    
    - An overall **Course Completion** percentage bar.
        
    - A dynamic **"Current Mastery"** score, averaging all individual Concept Mastery Scores.
        
    - A **Concept Heatmap** (see 6.3) providing an at-a-glance view of all concepts.
        
3. **Drill Down:** User hovers over a cell in the Heatmap to see the concept name and exact score, then clicks on it.
    
4. **Analyze Details:** The user is taken to a "Concept Detail View," which shows:
    
    - The current Mastery Score for that specific concept.
        
    - A trend line of the score over the last 15 relevant interactions.
        
    - A list of recent activities (assessments, scenarios) with their scores and dates.
        
5. **Take Action:** A "Next Steps" module provides 1-3 targeted recommendations based on their current status. The user can click a recommendation to navigate directly to the suggested activity.
    

### **6. Functional Requirements**

#### **6.1. Comprehensive Data Tracking**

- The system must track and store all relevant user learning data, including module completion, assessment scores (question-level), Tier 4 "Gist Ratings," and interaction with all Learning Tiers.
    
- Data must be tagged with the relevant Concept(s).
    
- **Active Engagement Time** per concept must be tracked, defined as time where the user is actively interacting with the page (mouse movement, clicks, keyboard input).
    

#### **6.2. Core Logic: Mastery Score Calculation (V1 Hypothesis)**

The system will calculate a "Mastery Score" for each Concept. This is the engine driving the analytics.

- **Formula:** The Mastery Score is a weighted average of all relevant performance data points for a given concept, with a time-decay factor applied.
    
- **Time Decay:** The weight of a data point decays exponentially based on its age. We will use a half-life of 21 days as a starting point. Weight = InitialWeight * (0.5 ^ (TimeInDays / 21))
    
- **Initial Data Point Weighting (Hypothesis for Validation):**
    
    - Tier 4 Gist Rating: 50%
        
    - Tier 2/3 Interactive Assessments: 40%
        
    - Tier 1 Quiz/Knowledge Checks: 10%
        
- **Score Initialization:** The first interaction with a concept sets the initial score. The score is then recalculated with every subsequent interaction related to that concept.
    
- **Mastery Status Thresholds:**
    
    - < 60%: **Needs Review** (Red)
        
    - 60% - 84%: **Progressing** (Yellow)
        
    - â‰¥ 85%: **Mastered** (Green)
        

#### **6.3. Visualizations & Reports**

- **Concept Heatmap:** A grid on the main dashboard where each cell represents a single Concept. The cell's color will directly correspond to its Mastery Status (Red, Yellow, Green).
    
- **Progress Trend Line:** In the "Concept Detail View," a line graph will plot the user's Mastery Score for that concept over time (last 15 data points) to visualize momentum. The x-axis will be event-based (e.g., "Interaction 1, 2, 3...") but will include date tooltips.
    

#### **6.4. Rules-Based Recommendations (V1)**

The system will provide recommendations using a defined, rules-based engine.

- **IF** Mastery Status is "Needs Review":
    
    - **THEN** recommend reviewing the foundational Tier 1 & 2 content for that concept.
        
- **IF** Mastery Status is "Progressing":
    
    - **AND** the user has not attempted a Tier 4 scenario for this concept: **THEN** recommend a Tier 4 practice scenario.
        
    - **AND** the user has attempted a Tier 4 scenario: **THEN** recommend a specific Tier 2/3 interactive exercise they previously scored low on, or a new one if available.
        
- **IF** Mastery Status is "Mastered":
    
    - **THEN** congratulate the user and recommend moving to the next concept in their learning path.
        

### **7. Validation Plan**

This feature's success rests on the validity of its core logic. The following steps will be taken to de-risk our assumptions.

- **Pre-Development:**
    
    - **Data Modeling:** The data science team will backtest the proposed Mastery Score formula against a sample of existing user data to check for anomalies and tune the initial time-decay half-life.
        
- **During Development:**
    
    - **Usability Testing:** The UX team will test clickable prototypes of the dashboard and detail views with at least 8 target users to ensure the visualizations are clear, trustworthy, and actionable.
        
- **Post-Launch:**
    
    - **A/B Testing:** We will launch the feature to 50% of new users. Success metrics will be measured by comparing the variant group against the control group.
        
    - **Ongoing Tuning:** The product and data teams will review the Mastery Score algorithm quarterly against user outcomes to identify opportunities for tuning weights and logic.
        

### **8. Success Metrics**

- **Adoption Rate:** Achieve a 40% adoption rate (users clicking into the "My Progress" section) within their first 30 days.
    
- **Causal Impact on Learning:** In a controlled A/B test, the user group with access to the analytics dashboard will achieve an average platform-wide Mastery Score that is **10% higher** than the control group over a 60-day period.
    
- **User Satisfaction:** Achieve a score of 4.5/5 or higher on user surveys asking: "How clear is your understanding of your learning progress using this feature?"
    

### **9. Out of Scope for V1**

- Advanced data export capabilities for individual users.
    
- Social or comparative features (e.g., benchmarking against peers).
    
- Machine learning-driven recommendations (this is a V2 goal).
    
- Formal reporting for external entities (e.g., employers).
    
- Direct access for users to raw analytics data beyond the UI.
    

### **10. Dependencies & Open Questions**

#### **10.1. Dependencies**

- **Dependency:** Accurate scoring and concept-tagging from the Performance Assessment Engine.
    
- **Dependency:** Availability of a robust data pipeline for processing interaction data.
    
- **Dependency:** Front-end components for data visualization must be available from the design system.
    

#### **10.2. Open Questions for Review**

- **For Engineering:** Is the proposed time-decay calculation computationally feasible to run in near real-time on every user interaction? What are the architectural implications?
    
- **For Design:** What is the best way to handle the "empty state" for a brand new user who has no progress data yet? How can we make that motivating?
    
- **For Data Science:** What is the minimum number of interactions required before we can consider a Mastery Score to be statistically significant and trustworthy to the user? Should we display a "Calibrating..." state?