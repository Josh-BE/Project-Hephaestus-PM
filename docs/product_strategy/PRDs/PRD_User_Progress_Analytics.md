### **PRD: User Progress Analytics**

### **1. Overview**

This document outlines the product requirements for the User Progress Analytics feature. This feature is pivotal in empowering learners by providing transparent, actionable insights into their comprehension, skill acquisition, and overall learning journey. By giving learners a clear, data-backed understanding of their progress, particularly their ability to practically apply knowledge, we aim to build profound confidence and foster a more effective, motivating learning environment.

### **2. The Problem**

Learners frequently struggle with ambiguity regarding their true mastery of a subject. Traditional metrics like chapter completion or simple test scores fail to pinpoint specific weaknesses, show trends over time, or build confidence in real-world capabilities. This leaves learners feeling uncertain about their competence, unmotivated by unseen progress, and unsure of the most effective next step to take. This feature directly attacks that uncertainty.

### **3. Key Terms & Definitions**

To ensure clarity across all teams, the following terms are defined:

- **Learning Tiers:** A classification of learning activities based on their cognitive purpose, moving a learner from foundational knowledge to real-world application.
    
    - **Tier 1 (Cued Recall):** Demonstrating foundational knowledge. This tier answers the question, "Do I know the basic facts?"
        
        - Examples: Foundational knowledge checks, flashcards.
            
    - **Tier 2 (Conceptual Understanding):** Explaining a concept in simple terms to prove comprehension. This tier answers the question, "Can I explain this to a five-year-old?"
        
        - Examples: Short-answer questions requiring interpretation, labeling interactive diagrams, concept mapping.
            
    - **Tier 3 (Context & Experimentation):** Understanding the "why" behind a concept and observing its effects. This tier answers the question, "Why does this matter, and how does it work in a controlled setting?"
        
        - Examples: Interactive exercises where users manipulate variables; explaining the practical importance of a concept.
            
    - **Tier 4 (Practical Application):** Applying knowledge to solve a realistic problem. This tier answers the question, "Can I actually use this?"
        
        - Examples: Completing a micro-scenario that simulates a real-world challenge, requiring the use of one or more concepts to achieve a goal.
            
- **Gist Rating:** A 1-10 score generated by our Performance Assessment Engine at the conclusion of a Tier 4 micro-scenario. It holistically measures how effectively the user accomplished the scenario's primary goal.
    
- **Concept:** The most granular unit of learning in our system (e.g., "Newton's Third Law," "CSS Flexbox," "Cross-Site Scripting"). All learning activities are tagged with one or more concepts.
    
- **Mastery Score:** A calculated, time-weighted score from 0-100 that represents our current best estimate of a learner's proficiency in a single Concept. **This is a V1 hypothesis and will be subject to continuous validation and tuning.**
    

### **4. Goals & Business Impact**

#### **4.1. Primary Goals**

- **User Insight:** Provide learners with transparent, actionable insights into their progress and mastery.
    
- **User Motivation:** Enhance user engagement and motivation by making progress tangible.
    
- **Platform Intelligence:** Fuel our adaptive learning engine with granular performance data.
    

#### **4.2. Business Impact**

- **Increase User Retention:** By clarifying the value of continued practice, we aim to increase 30-day active user retention.
    
- **Strengthen Market Position:** Serve as a key differentiator for achieving demonstrable competence.
    
- **Enable Premium Tiers:** The insights generated will be a cornerstone for future premium offerings.
    

### **5. User Stories & User Flow**

#### **5.1. Key User Stories**

- As a learner, I want to see my overall progress at a glance so that I can feel a sense of momentum.
    
- As a learner, I want to instantly identify my specific strengths and knowledge gaps so that I know what to focus on.
    
- As a learner, I want to see the evidence behind the system's assessment of my skills so that I can trust its analysis.
    
- As a learner, I want to get clear, actionable recommendations to improve efficiently.
    
- As a learner, I want to see my practical application ability (Gist Rating) prominently displayed to confirm my real-world competence.
    

#### **5.2. High-Level User Flow**

1. **Access Dashboard:** User clicks a "My Progress" link in the main navigation.
    
2. **View Overview:** The dashboard loads, displaying an overview with a Course Completion bar, a "Current Mastery" score, and a Concept Heatmap.
    
3. **Drill Down:** User clicks a cell in the Heatmap.
    
4. **Analyze Details:** The user is taken to a "Concept Detail View," which shows the specific Mastery Score, a trend line, and a list of recent activities.
    
5. **Take Action:** A "Next Steps" module provides targeted recommendations.
    

### **6. Functional Requirements**

#### **6.1. Comprehensive Data Tracking**

- The system must track and store all relevant learning data, tagged by Concept.
    
- Active Engagement Time per concept must be tracked.
    

#### **6.2. Core Logic: Mastery Score Calculation (V1 Hypothesis)**

- **Formula:** A weighted average of all relevant performance data points, with a time-decay factor applied.
    
- **Time Decay:** Exponential decay with a half-life of 21 days: Weight = InitialWeight * (0.5 ^ (TimeInDays / 21))
    
- **Initial Data Point Weighting (Hypothesis for Validation):**
    
    - Tier 4 Gist Rating: 50%
        
    - Tier 2/3 Interactive Assessments: 40%
        
    - Tier 1 Quiz/Knowledge Checks: 10%
        
- **Mastery Status Thresholds:** < 60% (Needs Review/Red), 60%-84% (Progressing/Yellow), â‰¥ 85% (Mastered/Green).
    

#### **6.3. Visualizations & Reports**

- **Concept Heatmap:** A grid on the main dashboard colored according to Mastery Status.
    
- **Progress Trend Line:** In the "Concept Detail View," a line graph plotting Mastery Score over the last 15 data points.
    

#### **6.4. Rules-Based Recommendations (V1)**

- **IF** "Needs Review": THEN recommend foundational Tier 1 & 2 content.
    
- **IF** "Progressing": THEN recommend a Tier 4 micro-scenario or a specific Tier 2/3 exercise based on past attempts.
    
- **IF** "Mastered": THEN congratulate and recommend the next concept.
    

### **7. Validation Plan**

- **Pre-Development:** Data modeling to backtest the Mastery Score formula.
    
- **During Development:** Usability testing of prototypes with at least 8 users.
    
- **Post-Launch:** A/B testing against a control group and quarterly reviews of the algorithm.
    

### **8. Success Metrics**

- **Adoption:** 40% of users click into the "My Progress" section within 30 days.
    
- **Causal Impact:** A/B test shows a 10% higher average Mastery Score for the variant group over 60 days.
    
- **Satisfaction:** Achieve 4.5/5 or higher on user surveys about progress clarity.
    

### **9. Out of Scope for V1**

- Data export, social comparison features, machine learning-driven recommendations, and formal external reporting.
    

### **10. Dependencies & Open Questions**

#### **10.1. Dependencies**

- Accurate scoring and tagging from the Performance Assessment Engine.
    
- Robust data pipeline and front-end visualization components.
    

#### **10.2. Open Questions for Review**

- **Engineering:** Is the real-time, time-decay calculation computationally feasible?
    
- **Design:** How do we handle the "empty state" for a new user in a motivating way?
    
- **Data Science:** Should we display a "Calibrating..." state until a minimum number of interactions is reached for a concept?